Scaling input data...
Max value: 255.0
Number of classes in this dataset: 10
One hot encoding targets...
Original input shape: (28, 28, 1)
2020-02-11 21:52:45.653527: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-02-11 21:52:45.669038: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fc1249996c0 executing computations on platform Host. Devices:
2020-02-11 21:52:45.669056: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       
_________________________________________________________________
activation_1 (Activation)    (None, 26, 26, 32)        0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 26, 26, 32)        128       
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 24, 24, 32)        9248      
_________________________________________________________________
activation_2 (Activation)    (None, 24, 24, 32)        0         
_________________________________________________________________
batch_normalization_2 (Batch (None, 24, 24, 32)        128       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 12, 12, 32)        0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 12, 12, 32)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 10, 10, 64)        18496     
_________________________________________________________________
activation_3 (Activation)    (None, 10, 10, 64)        0         
_________________________________________________________________
batch_normalization_3 (Batch (None, 10, 10, 64)        256       
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 8, 8, 64)          36928     
_________________________________________________________________
activation_4 (Activation)    (None, 8, 8, 64)          0         
_________________________________________________________________
batch_normalization_4 (Batch (None, 8, 8, 64)          256       
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 4, 4, 64)          0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 4, 4, 64)          0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 4, 4, 128)         73856     
_________________________________________________________________
activation_5 (Activation)    (None, 4, 4, 128)         0         
_________________________________________________________________
batch_normalization_5 (Batch (None, 4, 4, 128)         512       
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 4, 4, 128)         147584    
_________________________________________________________________
activation_6 (Activation)    (None, 4, 4, 128)         0         
_________________________________________________________________
batch_normalization_6 (Batch (None, 4, 4, 128)         512       
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 2, 2, 128)         0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 2, 2, 128)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 2048)              1050624   
_________________________________________________________________
activation_7 (Activation)    (None, 2048)              0         
_________________________________________________________________
dropout_4 (Dropout)          (None, 2048)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 10)                20490     
_________________________________________________________________
activation_8 (Activation)    (None, 10)                0         
=================================================================
Total params: 1,359,338
Trainable params: 1,358,442
Non-trainable params: 896
_________________________________________________________________
Train on 60000 samples, validate on 10000 samples
Epoch 1/10
 - 114s - loss: 0.5997 - accuracy: 0.8399 - val_loss: 0.7177 - val_accuracy: 0.7779
roc-auc: 0.995 - roc-auc_val: 0.9827                                                                                                    

Epoch 00001: val_loss improved from inf to 0.71775, saving model to /Users/mitsukakiyohara/GitHub/ATCS_y4_-mitsukakiyohara/Secondus/kuzushiji-MNIST/kmnist_weights.hdf5
Epoch 2/10
 - 109s - loss: 0.1807 - accuracy: 0.9456 - val_loss: 0.2660 - val_accuracy: 0.9327
roc-auc: 0.9997 - roc-auc_val: 0.9959                                                                                                    

Epoch 00002: val_loss improved from 0.71775 to 0.26595, saving model to /Users/mitsukakiyohara/GitHub/ATCS_y4_-mitsukakiyohara/Secondus/kuzushiji-MNIST/kmnist_weights.hdf5
Epoch 3/10
 - 107s - loss: 0.1343 - accuracy: 0.9607 - val_loss: 0.1765 - val_accuracy: 0.9521
roc-auc: 0.9999 - roc-auc_val: 0.998                                                                                                    

Epoch 00003: val_loss improved from 0.26595 to 0.17649, saving model to /Users/mitsukakiyohara/GitHub/ATCS_y4_-mitsukakiyohara/Secondus/kuzushiji-MNIST/kmnist_weights.hdf5
Epoch 4/10
 - 111s - loss: 0.1062 - accuracy: 0.9679 - val_loss: 0.2219 - val_accuracy: 0.9506
roc-auc: 0.9998 - roc-auc_val: 0.9976                                                                                                    

Epoch 00004: val_loss did not improve from 0.17649
Epoch 5/10
 - 106s - loss: 0.0971 - accuracy: 0.9716 - val_loss: 0.2009 - val_accuracy: 0.9531
roc-auc: 0.9999 - roc-auc_val: 0.9981                                                                                                    

Epoch 00005: val_loss did not improve from 0.17649
Epoch 6/10
 - 113s - loss: 0.0873 - accuracy: 0.9749 - val_loss: 0.2494 - val_accuracy: 0.9464
roc-auc: 0.9999 - roc-auc_val: 0.9978                                                                                                    

Epoch 00006: val_loss did not improve from 0.17649
Epoch 7/10
 - 108s - loss: 0.0872 - accuracy: 0.9761 - val_loss: 0.1883 - val_accuracy: 0.9573
roc-auc: 0.9999 - roc-auc_val: 0.9985                                                                                                    

Epoch 00007: val_loss did not improve from 0.17649
Epoch 8/10
 - 108s - loss: 0.0763 - accuracy: 0.9790 - val_loss: 0.1709 - val_accuracy: 0.9639
roc-auc: 1.0 - roc-auc_val: 0.9987                                                                                                    

Epoch 00008: val_loss improved from 0.17649 to 0.17085, saving model to /Users/mitsukakiyohara/GitHub/ATCS_y4_-mitsukakiyohara/Secondus/kuzushiji-MNIST/kmnist_weights.hdf5
Epoch 9/10
 - 112s - loss: 0.0736 - accuracy: 0.9794 - val_loss: 0.2086 - val_accuracy: 0.9625
roc-auc: 1.0 - roc-auc_val: 0.9986                                                                                                    

Epoch 00009: val_loss did not improve from 0.17085
Epoch 10/10
 - 116s - loss: 0.0724 - accuracy: 0.9807 - val_loss: 0.1727 - val_accuracy: 0.9649
roc-auc: 1.0 - roc-auc_val: 0.9988                                                                                                    

Epoch 00010: val_loss did not improve from 0.17085

Test accuracy: 0.964900016784668
