diff --git a/SamplePyAdvPracTester.py b/SamplePyAdvPracTester.py
index 57c4e7f..3bbc1c3 100644
--- a/SamplePyAdvPracTester.py
+++ b/SamplePyAdvPracTester.py
@@ -3,6 +3,7 @@
 import math
 f=0 
 extra=0
+num_exer=5
 
 try:
     import PythonAdvPractice_2019 as pap
@@ -106,6 +107,103 @@ try:
         print("FAILED: list_overlap_comp threw an exception.")
         f += 1
 
+# More practice with Dictionaries, Files, and Text!
+# Implement the following functions:
+
+    test_files = []
+    test_files.append("rj_prologue.txt")
+    test_files.append("permutation.txt")
+    test_files.append("UncannyValley.txt")
+    
+# def longest_sentence(text_file_name):
+    """ Read from the text file, split the data into sentences,
+        and return the longest sentence in the file.
+    """
+    answers = []
+    answers.append("The fearful passage of their death-mark'd love,\nAnd the continuance of their parents' rage,\nWhich, but their children's end, nought could remove,\nIs now the two hours' traffic of our stage")
+    answers.append("Objects out of sight didn't \"vanish\" entirely, if they influenced the ambient light, but Paul knew that the calculations would rarely be pursued beyond the crudest first-order approximations: Bosch's Garden of Earthly Delights reduced to an average reflectance value, a single grey rectangle - because once his back was turned, any more detail would have been wasted")
+    answers.append("Later, in a room of his own, his bed had come with hollow metal posts whose plastic caps were easily removed, allowing him to toss in chewed pencil stubs, pins that had held newly bought school shirts elaborately folded around cardboard packaging, tacks that he'd bent out of shape with misaligned hammer blows while trying to form pictures in zinc on lumps of firewood, pieces of gravel that had made their way into his shoes, dried snot scraped from his handkerchief, and tiny, balled-up scraps of paper, each bearing a four- or five-word account of whatever seemed important at the time, building up a record of his life like a core sample slicing through geological strata, a find for future archaeologists far more exciting than any diary")
+    try:
+        for i in range(len(test_files)):
+            output = pap.longest_sentence(test_files[i])
+            if  output.strip().lower().rstrip('.!?;') != answers[i].strip().lower().rstrip('.!?;'):
+                print("FAILED: longest_sentence(" + test_files[i] + ") returned: \n" + str(output) + "\n instead of: \n" + str(answers[i]))
+                f += 0.333333333
+            else:
+                print("passed: longest_sentence(" + test_files[i] + ") with: \n" + str(output))
+    except Exception as ex:
+        print(ex)
+        print("FAILED: longest_sentence threw an exception.")
+        f += 1
+
+
+# def longest_word(text_file_name):
+    """ Read from the text file, split the data into words,
+        and return the longest word in the file.
+    """
+    answers = []
+    answers.append("misadventured")
+    answers.append("soon-to-be-forgotten")
+    answers.append("jurisprudentially")
+
+    try:
+        for i in range(len(test_files)):
+            output = pap.longest_word(test_files[i])
+            if len(output) != len(answers[i]):
+                print("FAILED: longest_word(" + test_files[i] + ") returned: " + str(output) + " instead of: " + str(answers[i]))
+                f += 0.333333333
+            else:
+                print("passed: longest_word(" + test_files[i] + ") with: " + str(output))
+    except Exception as ex:
+        print(ex)
+        print("FAILED: longest_word threw an exception.")
+        f += 1
+
+# def num_unique_words(text_file_name):
+    """ Read from the text file, split the data into words,
+        and return the number of unique words in the file.
+        HINT: Use a set!
+    """
+    answers = []
+    answers.append(80)
+    answers.append(1540)
+    answers.append(2962)
+
+    try:
+        for i in range(len(test_files)):
+            output = pap.num_unique_words(test_files[i])
+            if math.fabs(output - answers[i]) > max(2,answers[i]/100):
+                print("FAILED: num_unique_words(" + test_files[i] + ") returned: " + str(output) + " instead of: " + str(answers[i]))
+                f += 0.333333333
+            else:
+                print("passed: num_unique_words(" + test_files[i] + ") with: " + str(output))
+    except Exception as ex:
+        print(ex)
+        print("FAILED: num_unique_words threw an exception.")
+        f += 1
+
+# def most_frequent_word(text_file_name):
+    """ Read from the text file, split the data into words,
+        and return a tuple with the most frequently occuring word 
+        in the file and the count of the number of times it apapared.
+    """
+    answers = []
+    answers.append(('their',6))
+    answers.append(('the', 266))
+    answers.append(('the', 720))
+    try:
+        for i in range(len(test_files)):
+            output = pap.most_frequent_word(test_files[i])
+            if  output[0].lower() != answers[i][0].lower() and math.fabs(output[1] - answers[i][1]) > max(2,answers[i][1]/100) :
+                print("FAILED: most_frequent_word(" + test_files[i] + ") returned: " + str(output) + " instead of: " + str(answers[i]))
+                f += 0.333333333
+            else:
+                print("passed: most_frequent_word(" + test_files[i] + ") with: " + str(output))
+    except Exception as ex:
+        print(ex)
+        print("FAILED: most_frequent_word threw an exception.")
+        f += 1
+
 except Exception as ex:
     print(ex)
     print("FAILED: PythonAdvPractice2019.py file does not execute at all, or this file was not implemented.")
@@ -113,6 +211,5 @@ except Exception as ex:
 
 print("\n")
 print("SUMMARY:")
-print("Passed " + str(round(2-f,2)) + " out of 2 exercises.")
-print("Earned " + str(round(extra,2)) + " extra credits.")
+print("Passed " + str(round(num_exer-f,2)) + " out of " + str(num_exer) + " exercises.")
 
diff --git a/Secondus/.DS_Store b/Secondus/.DS_Store
index 4025a28..9e31389 100644
Binary files a/Secondus/.DS_Store and b/Secondus/.DS_Store differ
diff --git a/Secondus/kuzushiji-MNIST/.DS_Store b/Secondus/kuzushiji-MNIST/.DS_Store
index 0008d27..1c03729 100644
Binary files a/Secondus/kuzushiji-MNIST/.DS_Store and b/Secondus/kuzushiji-MNIST/.DS_Store differ
diff --git a/Secondus/kuzushiji-MNIST/secondus.py b/Secondus/kuzushiji-MNIST/secondus.py
index ddaf093..d6184cc 100644
--- a/Secondus/kuzushiji-MNIST/secondus.py
+++ b/Secondus/kuzushiji-MNIST/secondus.py
@@ -85,7 +85,7 @@ model = Sequential()
 
 
 #1st Case: Running a CNN [Using a similar code to Assignment Quebec]
-
+np.randomseed(100)
 model.add(Conv2D(32, (3,3), padding='valid', input_shape=x_train.shape[1:]))
 model.add(Activation('elu'))
 model.add(BatchNormalization())
@@ -126,17 +126,28 @@ model.summary()
 from keras.optimizers import SGD, Adagrad, RMSprop, Adam, Nadam
 
 mloss = 'categorical_crossentropy'
-opt = 'Adam' 
+opt = 'Adagrad' 
 
 model.compile(loss=mloss, optimizer=opt, metrics=['accuracy'])
 
 epochs = 10
 
-#history = model.fit(x_train, y_train, epochs=epochs,verbose=2, validation_data=(x_test, y_test), shuffle=True)
+"""
+history = model.fit(x_train, y_train,
+                    epochs=epochs,
+                    verbose=2,
+                    validation_data=(x_test, y_test),
+              		shuffle=True,
+                    callbacks=[WandbCallback()],
+                    use_multiprocessing=True)
+"""
 
 #score = model.evaluate(x_test, y_test, verbose=0)
 #print('\nTest accuracy:', score[1])
 
+# Save results to wandb directory
+#model.save(os.path.join(wandb.run.dir, "model.h5"))
+
 
 #3rd case: running a normal CNN but implementing callbacks 
 """
@@ -151,13 +162,16 @@ from sklearn.model_selection import train_test_split
 from sklearn.metrics import classification_report, roc_auc_score, accuracy_score
 from sklearn.metrics import confusion_matrix
 from tensorflow.keras.callbacks import Callback
+
+#2nd case: running a normal CNN using only 10 layers
 """
-2nd case: running a normal CNN using only 10 layers
 act = 'relu'
 epochs = 100
 opt = 'Adam'
 mloss = 'categorical_crossentropy'
 
+np.randomseed(512)
+
 model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(img_rows, img_cols, 1)))
 model.add(Conv2D(32, (3, 3), activation='relu'))
 model.add(MaxPooling2D(pool_size=(2, 2)))
@@ -171,12 +185,11 @@ model.summary()
 
 #Compiling [Optimizer: Adam, Loss: Categorical Crossentropy]
 model.compile(loss=mloss, optimizer=opt, metrics=['accuracy'])
-"""
-"""
-Callback for loss history 
 
-Credits: https://github.com/netsatsawat
 """
+#auc_loss_history Class comes from: 
+#Credits to: https://github.com/netsatsawat
+
 class auc_loss_history(Callback):
     def __init__(self, training_data, validation_data):
         self.x = training_data[0]
@@ -195,11 +208,11 @@ class auc_loss_history(Callback):
         self.val_losses = []
         
     def _auc_score(self, predictions, targets):
-        """
-        Function to compute Compute AUC ROC Score
-        """
-        return roc_auc_score(predictions, targets)
-        
+    	"""
+    	Function to compute Compute AUC ROC Score
+    	"""
+    	return roc_auc_score(predictions, targets)
+
     def on_train_end(self, logs={}):
         return
     
@@ -222,14 +235,14 @@ class auc_loss_history(Callback):
     
     def on_batch_end(self, batch, logs={}):
         return
-
-
+"""
+"""
 #Defining Callbacks 
 auc_loss_hist = auc_loss_history(training_data=(x_train, y_train),
                            validation_data=(x_test, y_test))
 checkpointer = ModelCheckpoint(filepath='/Users/mitsukakiyohara/GitHub/ATCS_y4_-mitsukakiyohara/Secondus/kuzushiji-MNIST/kmnist_weights.hdf5', verbose=1, 
                                monitor='val_loss',save_best_only=True, mode='min')
-early_stop = EarlyStopping(monitor='val_loss', patience=10, mode='min')
+early_stop = EarlyStopping(monitor='val_loss', patience=5, mode='min')
 
 #Fitting the Model
 hist = model.fit(x_train, y_train,
@@ -243,6 +256,7 @@ hist = model.fit(x_train, y_train,
 
 
 # Add WandbCallback() to callbacks
+
 """
 history = model.fit(x_train, y_train,
                     epochs=epochs,
@@ -251,6 +265,7 @@ history = model.fit(x_train, y_train,
               		shuffle=True,
                     callbacks=[WandbCallback()],
                     use_multiprocessing=True)
+
 """
 
 score = model.evaluate(x_test, y_test, verbose=0)
diff --git a/oscar.py b/oscar.py
index 8c03265..084b663 100644
--- a/oscar.py
+++ b/oscar.py
@@ -7,6 +7,7 @@ __author__   = "Mitsuka Kiyohara"
 
 #Load in data set and name columns
 mushroom_data = pd.read_csv("mushrooms.csv", header=None, names=['E/P','cap-shape','cap-surface','cap-color','bruises','odor','gill-attachment','gill-spacing','gill-size','gill-color','stalk-shape','stalk-root','stalk-surface-above-ring','stalk-surface-below-ring','stalk-color-above-ring','stalk-color-below-ring','veil-type','veil-color','ring-number','ring-type','spore-print-color','population','habitat']) 
+
 mushroom_targets = mushroom_data[['E/P']]
 
 
@@ -138,7 +139,7 @@ for model in models:
     
     for encode in encoders:
         print("Testing with: ", encode)
-        #encode data 
+        #encode data  
         new_mushroom_data = pd.DataFrame(encode.fit_transform(train_inputs).toarray())
         train_inputs = new_mushroom_data
  
@@ -156,6 +157,7 @@ for model in models:
     
     
     
+    
 
 
 
