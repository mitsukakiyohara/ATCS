{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Neural Networks with Keras - Part One\n",
    "# Covers basic layer construction and how to train and predict with NNs.\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "### First, pick and prepare a dataset:\n",
    "# (comment/uncomment one of the following):\n",
    "\n",
    "\n",
    "# Fashion MNIST\n",
    "# 60000/10000 32x32 black and white images of clothing items.\n",
    "from keras.datasets import fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# CIFAR10\n",
    "# 50000/10000 32x32 color images of various real-world items (cars, ships, etc...)\n",
    "# https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "from keras.datasets import cifar10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\"\"\"\n",
    "\n",
    "# Preparing the data:\n",
    "print(\"Scaling input data...\")\n",
    "max_val = np.max(x_train).astype(np.float32)\n",
    "print(\"Max value: \" +  str(max_val))\n",
    "x_train = x_train.astype(np.float32) / max_val\n",
    "x_test = x_test.astype(np.float32) / max_val\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "#create range from 0 to 1\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "num_classes = len(np.unique(y_train))\n",
    "print(\"Number of classes in this dataset: \" + str(num_classes))\n",
    "if num_classes > 2:\n",
    "\tprint(\"One hot encoding targets...\")\n",
    "\ty_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "\ty_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(\"Original input shape: \" + str(x_train.shape[1:]))\n",
    "\n",
    "### Second, build a model:\n",
    "\n",
    "\"\"\"\n",
    "For standard Feed Forward Deep Networks we will use Dense layers in a Sequential model:\n",
    "\n",
    "Example:\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(...,...)))\n",
    "model.add(Dense(300, activation=act, kernel_initializer=init))\n",
    "etc...\n",
    "\n",
    "The most important input parameters are activation and initialization.\n",
    "Some combination choices are shown here:\n",
    "\n",
    "Activation         Initialization\n",
    "----------         --------------\n",
    "sigmoid            glorot_normal or glorot_uniform\n",
    "tanh\t\t\t   glorot_normal or glorot_uniform\n",
    "relu               he_normal or he_uniform\n",
    "*leaky relu        he_normal or he_uniform\n",
    "elu                he_normal or he_uniform\n",
    "selu               lecun_normal or lecun_uniform\n",
    "\n",
    "*Leaky ReLU is implemented as a custom function\n",
    "\n",
    "A full list of activation functions can be found here:\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/activations/\n",
    "\n",
    "A full list of initializers available in keras can be found here:\n",
    "https://keras.io/initializers/\n",
    "\n",
    "A list of the other input parameters for the Dense layer are here:\n",
    "https://keras.io/layers/core/\n",
    "\n",
    "The last layer is the output layer, and should be configured based on\n",
    "the kind of problem you are trying to solve:\n",
    "\n",
    "Regression- one node, linear activation function (which is the default)\n",
    "Binary Classification - one node, sigmoid activation function\n",
    "Multi-Class Classification - num nodes = classes, softmax activation function\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def leakyReLU(z, name=None):\n",
    "    return tf.maximum(0.01 * z, z, name=name)\n",
    "\n",
    "## Choose activation and initialization functions.\n",
    "# Set equal to a string with the name of the activation or initialization\n",
    "#    function you want to use, except for Leaky ReLU. For that set equal\n",
    "#    to the name of the function, without quotes.\n",
    "\n",
    "act = 'sigmoid'\n",
    "init = 'glorot_normal'\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=x_train.shape[1:]))\n",
    "model.add(Dense(300, activation=act, kernel_initializer=init))\n",
    "model.add(Dense(100, activation=act, kernel_initializer=init))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "### Third, compile the model with a loss function, optimizer algorithm, and metrics\n",
    "\n",
    "\"\"\"\n",
    "Loss functions depend on the task you are training the NN to do:\n",
    "\n",
    "Regression tasks-\n",
    "\tmean_squared_error  # recommended default choice\n",
    "\tmean_absolute_error  # for problematic outliers\n",
    "\tmean_squared_logarithmic_error  # for a very wide range of targets\n",
    "\n",
    "Binary Classification tasks-\n",
    "\tbinary_crossentropy # recommended default choice; targets: 0,1\n",
    "\thinge # SVM approach (mixed results with NN); targets: -1,1; use tanh for output layer activation\n",
    "\tsquared_hinge # smoother hinge loss function, targets: -1,1; use tanh for output layer activation\n",
    "\n",
    "Multi-class Classification tasks-\n",
    "\tcategorical_crossentropy # recommended default choice; use keras.utils.to_categorical(...) on targets\n",
    "\tsparse_categorical_crossentropy # for a large number of targets; to_categorical not required on targets\n",
    "\t\n",
    "\n",
    "For more loss function choices:  https://keras.io/losses/\n",
    "\n",
    "Some Optimizer choices:\n",
    "Gradient Descent\t\tSGD(lr=0.01)\n",
    "Momentum\t\t\t\tSGD(lr=0.01, momentum=0.9)\n",
    "Nesterov momentum\t\tSGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "AdaGrad\t\t\t\t\tAdagrad()\n",
    "RMSprop                 RMSprop()\n",
    "Adam\t\t\t\t\tAdam() \n",
    "Nesterov Adam\t\t\tNadam()\n",
    "\n",
    "For a full list of optimizers:  https://keras.io/optimizers/\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "from keras.optimizers import SGD, Adagrad, RMSprop, Adam, Nadam\n",
    "\n",
    "mloss = 'categorical_crossentropy'\n",
    "opt = SGD(lr=0.01)\n",
    "\n",
    "model.compile(loss=mloss,\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "### Fourth, train and test the model!\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=epochs,\n",
    "                    verbose=2,\n",
    "                    validation_data=(x_test, y_test),\n",
    "              \t\tshuffle=True)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('\\nTest accuracy:', score[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
